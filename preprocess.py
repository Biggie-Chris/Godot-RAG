import os
import re
import json
import concurrent.futures
from typing import Dict, List, Tuple

from tqdm import tqdm
import tiktoken
import chromadb
from pathlib import Path


from Embeddings import get_embeddings

# 1.é…ç½®è·¯å¾„
DOC_DIR = "doc"
DOC_SOURCE_DIR = os.path.join(DOC_DIR, "_sources")
SEARCH_INDEX_JS = os.path.join(DOC_DIR, "searchindex.js")

DATA_DIR = "data"
CHUNKS_DIR = os.path.join(DATA_DIR, "chunks")
META_DIR = os.path.join(DATA_DIR, "meta")
VECTORDB_DIR = os.path.join(DATA_DIR, "VectorDB")

CHUNKS_FILE = os.path.join(CHUNKS_DIR, "chunks.jsonl")
SEARCHINDEX_JSON = os.path.join(META_DIR, "searchindex.json")

# 2.é…ç½®åˆ†è¯å™¨
enc = tiktoken.get_encoding("cl100k_base")

def ensure_dirs() -> None:
    """åˆ›å»ºæ‰€éœ€ç›®å½•"""
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(CHUNKS_DIR, exist_ok=True)
    os.makedirs(META_DIR, exist_ok=True)
    os.makedirs(VECTORDB_DIR, exist_ok=True)
    print("âœ… æ‰€æœ‰å¿…è¦çš„ç›®å½•å·²åˆ›å»º")
    

# 3.è§£æsearchindex.js
def parse_searchindex() -> Dict:
    """
    å°† searchindex.js è§£æä¸º JSON å¹¶ä¿å­˜ã€‚
    è¿”å›è§£æåçš„ dict
    """
    text = Path(SEARCH_INDEX_JS).read_text(encoding="utf-8")

    start = text.find("Search.setIndex(")
    if start == -1:
        raise RuntimeError("åœ¨ searchindex.js ä¸­æœªæ‰¾åˆ° Search.setIndex(...) ç»“æ„")

    start = text.find("{", start)
    if start == -1:
        raise RuntimeError("Search.setIndex åæ‰¾ä¸åˆ° JSON èµ·å§‹ {")

    # åŒ¹é… JSON èŠ±æ‹¬å·
    brace_count = 0
    end = start

    for i, ch in enumerate(text[start:], start=start):
        if ch == "{":
            brace_count += 1
        elif ch == "}":
            brace_count -= 1
            if brace_count == 0:
                end = i
                break

    json_text = text[start:end + 1]

    # è§£æ JSON
    data = json.loads(json_text)

    Path(SEARCHINDEX_JSON).write_text(
        json.dumps(data, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    print(f"âœ… å·²å†™å…¥ {SEARCHINDEX_JSON}")

    return data

# 4.å»ºç«‹ id å’Œ æ–‡æ¡£åçš„æ˜ å°„æ–‡ä»¶
def build_source_meta(searchindex: Dict) -> Dict[str, Dict]:
    """
    åŸºäº searchindex.jsonï¼Œæ„å»º doc_id â†’ æºæ–‡ä»¶è·¯å¾„ çš„æ˜ å°„ã€‚
    Sphinx çš„ searchindex.js é‡Œ docnames çš„é¡ºåºä¸æ–‡æ¡£ ID ä¸€ä¸€å¯¹åº”ã€‚
    """
    """
    æ ¹æ® searchindex.json ç”Ÿæˆï¼š
    {
        "0": { "name": "404", "source": "doc/_sources/404.txt" },
        ...
    }
    """

    print("ğŸ“Œ æ„å»º source meta ...")

    src_map = {}

    # searchindex["filenames"] å­˜çš„å°±æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œæ¯”å¦‚ï¼š
    # ["404", "about/complying_with_licenses", ...]
    filenames = searchindex.get("filenames", [])

    for idx, rel in enumerate(filenames):
        # æ„é€ çœŸå® txt æ–‡ä»¶è·¯å¾„
        txt_path = Path(DOC_SOURCE_DIR) / (rel + ".txt")

        src_map[str(idx)] = {
            "name": rel,
            "source": str(txt_path),
        }

    # ä¿å­˜ meta
    Path(os.path.join(META_DIR, "source_meta.json")).write_text(
        json.dumps(src_map, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

    print("âœ… source_meta.json å·²ç”Ÿæˆ")

    return src_map
    
# 5. éå†æ‰€æœ‰ txt æ–‡ä»¶ï¼Œå¹¶ä¸”ç”Ÿæˆ chunks
def iter_source_txt_files(root: str):
    """éå† DOC_SOURCE_DIR ä¸‹çš„æ‰€æœ‰ .txt æ–‡ä»¶"""
    for dirpath, _, filenames in os.walk(root):
        for fname in filenames:
            if fname.endswith(".txt"):
                yield os.path.join(dirpath, fname)

def chunk_text(
    text: str,
    max_tokens: int = 600,
    overlap_tokens: int = 150,
) -> List[str]:
    
    tokens = enc.encode(text)
    chunks = []

    start = 0
    n = len(tokens)

    while start < n:
        end = min(start + max_tokens, n)
        chunk_tokens = tokens[start:end]
        chunk_text = enc.decode(chunk_tokens)
        chunks.append(chunk_text)

        # ä¸‹ä¸€æ®µèµ·ç‚¹ = å½“å‰ start + (max_tokens - overlap_tokens)
        start += max_tokens - overlap_tokens

    return chunks

def generate_chunks(source_meta_map: Dict[str, Dict]) -> None:
    """ä»æ‰€æœ‰æº TXT ç”Ÿæˆæ–‡æœ¬ chunk å¹¶å†™å…¥ chunks.jsonl"""
    
    """"
    æ¯ä¸ª chunk çš„æ ¼å¼: {"id": "0_3", "doc_id": "0", "text": "...å†…å®¹...", "source": "doc/_sources/xxx.txt"}
    """
    fout = open(CHUNKS_FILE, "w", encoding="utf-8")
    print("å¼€å§‹ç”Ÿæˆ chunks ...")

    for doc_id, info in tqdm(source_meta_map.items()):
        # ä½¿ç”¨ Path æ„é€ çœŸå®æ–‡ä»¶è·¯å¾„ï¼Œé¿å…å­—ç¬¦ä¸²é”™è¯¯
        src_file = Path(info["source"])  

        if not src_file.exists():
            print(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{src_file}")
            continue

        text = src_file.read_text(encoding="utf-8")

        chunks = chunk_text(text)

        for idx, chunk in enumerate(chunks):
            record = {
                "id": f"{doc_id}_{idx}",
                "doc_id": doc_id,
                "text": chunk,
                "source": str(src_file),   # ä¿ç•™åŸå§‹å­—ç¬¦ä¸²æ ¼å¼
            }
            fout.write(json.dumps(record, ensure_ascii=False) + "\n")

    fout.close()
    print(f"âœ… å·²ç”Ÿæˆ chunks åˆ° {CHUNKS_FILE}")
    
def build_chroma_from_chunks() -> None:
    """è¯»å– chunks.jsonlï¼Œæ„å»º ChromaDB å‘é‡æ•°æ®åº“ï¼ˆæ‰¹é‡è°ƒç”¨ç¡…åŸºæµåŠ¨ embeddingï¼‰"""

    print("ğŸ“Œ å¼€å§‹æ„å»º ChromaDB ...")

    client = chromadb.PersistentClient(path=VECTORDB_DIR)

    collection = client.get_or_create_collection(
        name="godot_docs",
        metadata={"hnsw:space": "cosine"},
    )

    # å…ˆæŠŠæ‰€æœ‰è¡Œè¯»è¿›å†…å­˜ï¼Œæ–¹ä¾¿æ‰¹é‡å¤„ç†
    with open(CHUNKS_FILE, "r", encoding="utf-8") as f:
        lines = [line for line in f if line.strip()]

    total = len(lines)
    print(f"éœ€è¦ç”Ÿæˆå‘é‡çš„ chunks æ•°é‡ï¼š{total}")

    batch_size = 32  

    from math import ceil
    num_batches = ceil(total / batch_size)

    for bi in tqdm(range(num_batches), desc="Embedding (batch)"):
        start = bi * batch_size
        end = min(start + batch_size, total)
        batch_lines = lines[start:end]

        ids = []
        texts = []
        metas = []

        for line in batch_lines:
            item = json.loads(line)
            ids.append(item["id"])
            texts.append(item["text"])
            metas.append({"source": item["source"], "doc_id": item["doc_id"]})

        # è°ƒç”¨ç¡…åŸºæµåŠ¨æ‰¹é‡ embedding
        try:
            vectors = get_embeddings(texts)
        except Exception as e:
            print(f"âŒ æ‰¹é‡ embedding å‡ºé”™ï¼Œç¬¬ {bi} æ‰¹ï¼Œè·³è¿‡ã€‚é”™è¯¯ï¼š{e}")
            continue

        # å†™å…¥ ChromaDB
        collection.add(
            ids=ids,
            documents=texts,
            embeddings=vectors,
            metadatas=metas,
        )

    print("ChromaDB æ„å»ºå®Œæˆ")


def main():
    ensure_dirs()
    
    # searchindex =  parse_searchindex()
    # meata_map = build_source_meta(searchindex)
    # generate_chunks(meata_map)
    build_chroma_from_chunks()
    
if __name__ == "__main__":
    main()